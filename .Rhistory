local$eigenvector_centrality$rank <- order(local$eigenvector_centrality$score, decreasing = T)
# return a list object with a $global and $local sublist, each of which contains the output from all of the different measures which are appropriately named. In the $local sublist, we provide the $rank and $score for each node in the network.
return_list <- list(global = global, local = local)
return(return_list)
}
#######################################################################################
#Landau's h-outputs global stat only works for directed graphs
landau <- function(matrix,directed=TRUE){
if(directed==FALSE){
print("error: this measure may only be used with directed networks")
break;
}
N=dim(matrix)[1]
S=apply(matrix,1,sum)
sum(S-((N-1)/2))
h=12/(N^3-N) * sum(S-((N-1)/2))
results=list(global=h,local=rep(NA,N))
return(results)
}
#######################################################################################
#Kendall's K-outputs global stat only works for directed graphs
kendall <- function(matrix,directed=TRUE){
if(directed==FALSE){
print("error: this measure may only be used with directed networks")
break;
}
N=dim(matrix)[1]
S=apply(matrix,1,sum)
d=(N*(N-1)*(2*N-1))/12-(0.5*sum(S^2))
if((N%%2)==0){
#even
d_max=(1/24)*(N^3-4*N)
} else{
#odd
d_max=(1/24)*(N^3-N)
}
K=1-(d/d_max)
results=list(global=K,local=rep(NA,N))
return(results)
}
#######################################################################################
#m reach degree-requires the keyplayer package in R; outputs local stats
m_degree <- function(matrix,directed=TRUE){
require("keyplayer")
l=mreach.degree(matrix,cmode="outdegree")
results=list(global=NA,local=l)
return(results)
}
#######################################################################################
#m reach closeness-requires the keyplayer package in R; outputs local stats
m_close <- function(matrix,directed=TRUE){
require("keyplayer")
l=mreach.closeness(matrix,cmode="outdegree")
results=list(global=NA,local=l)
return(results)
}
#######################################################################################
#GRC-requires the keyplayer package in R; outputs global and local stats
GRC <- function(matrix,directed=TRUE){
require("keyplayer")
N=dim(matrix)[1]
if(directed==TRUE){
C=mreach.degree(matrix,cmode="outdegree")
}
if(directed==FALSE){
C=mreach.degree(matrix,cmode="all")[,3]
}
C_max=max(C)
GRC=(sum(C_max-C))/(N-1)
results=list(global=GRC,local=C)
return(results)
}
#######################################################################################
#Rooted Depth-requires the igraph package in R; outputs global and local stats
D_root <- function(matrix,directed=TRUE){
require(igraph)
if(directed==FALSE){
print("error: this measure may only be used with directed networks")
break;
}
roots=which(apply(matrix,1,sum)==0)
if(length(roots)==0){
print("error:There are no roots in your network")
break;
}
N=dim(matrix)[1]
graph=graph_from_adjacency_matrix(matrix,mode="directed")
paths=as.matrix(shortest.paths(graph,v=roots))
l=apply(paths,2,mean)
if(l[1]==Inf) {l=rep(NA,length(l))}
D_root=mean(as.vector(paths))
if(D_root==Inf){D_root=NA}
results=list(global=D_root,local=l)
return(results)
}
calculate_analytical_hierarhy_measures <- function(sociomatrix,
mode = "directed"){
#statistics using igraph
require(igraph)
sociomatrix <- graph.adjacency(sociomatrix, mode = mode)
#calculate global scores
global <- list()
global$degree_centralization <- centralization.degree (sociomatrix, mode = "all")$centralization
global$closeness_centralization <- centralization.closeness (sociomatrix, mode = "all")$centralization
global$betweenness_centralization <-  centralization.betweenness (sociomatrix, directed = TRUE)$centralization
global$eigenvector_centralization <- centralization.evcent (sociomatrix, directed = TRUE)$centralization
#calculate local scores
local <- list()
local$degree_centrality$score <- centralization.degree (sociomatrix, mode = "all")$res
local$degree_centrality$rank <- order(local$degree_centrality$score, decreasing = T)
local$closeness_centrality$score <- centralization.closeness (sociomatrix, mode = "all")$res
local$closeness_centrality$rank <- order(local$closeness_centrality$score, decreasing = T)
local$betweenness_centrality$score <-  centralization.betweenness (sociomatrix, directed = TRUE)$res
local$betweenness_centrality$rank <- order(local$betweenness_centrality$score, decreasing = T)
local$eigenvector_centrality$score <- centralization.evcent (sociomatrix, directed = TRUE)$vector
local$eigenvector_centrality$rank <- order(local$eigenvector_centrality$score, decreasing = T)
# return a list object with a $global and $local sublist, each of which contains the output from all of the different measures which are appropriately named. In the $local sublist, we provide the $rank and $score for each node in the network.
return_list <- list(global = global, local = local)
return(return_list)
}
data=read.csv("C:\\Users\\admin-ccook\\Desktop\\save_G_Net.csv",header=T)
data=read.csv("C:\\Users\\admin-ccook\\Desktop\\save_G_Net.csv",header=T)
pairs(data[,-c(1,4,5,7)])
pairs(data[,-c(1,4,5,7)],cex.labels=2)
#calculate measures for all networks
#works on Matt's Computer
#preliminaries
rm(list = ls())
#load in functions
source("./Scripts/calculate_analytical_hierarchy_measures.R")
source("./Scripts/score_leadership_rank.R")
source("./Scripts/multi_plot.R")
#load data
load("./Data/Network_Data.Rdata")
# calculate measures for all networks
Measures <- vector(length = length(Network_Data), mode = "list")
for(i in 1:length(Network_Data)){
print(i)
Measures[[i]] <- calculate_analytical_hierarhy_measures(
sociomatrix = Network_Data[[i]]$sociomatrix,
mode = Network_Data[[i]]$mode
)
}
#populate dataframe with global measures
global_measures <- data.frame(matrix(0,
nrow = length(Measures),
ncol = length(Measures[[1]]$global)))
for(i in 1:length(Measures)){
global_measures[i,] <- unlist(Measures[[i]]$global)
}
colnames(global_measures) <- names((Measures[[1]]$global))
rownames(global_measures) <- names(Network_Data)
save(global_measures, file = "./Data/global_hierarchy_measures.Rdata")
multi_plot(data = global_measures,
pdf_name = "Global_Measures",
output_pdf = F,
2:4)
#now score against
measure_scores <- score_leadership_rank(Network_Data = Network_Data,
Measures = Measures)
multi_plot(data = measure_scores,
pdf_name = "Measure_Scores",
output_pdf = T)
colMeans(measure_scores[1:17,])
colMeans(measure_scores[1:29,])
library(ggplot2)
library(tm)
library(MCMCpack)
install.packages("MCMCpack")
simulateCorpus <- function(
M, # number of documents
nTerms,
docLengths,
K,  	# Number of Topics
alphA, 	# parameter for symmetric
# Document/Topic dirichlet distribution
betA, 	# parameter for Topic/Term dirichlet distribution
Alpha=rep(alphA,K), # number-of-topics length vector
# set to symmetric alpha parameter
# across all topics
Beta=rep(betA,nTerms))  # number-of-terms length vector
# set to symmetric beta parameter
# across all terms
{
# Labels
Terms <- paste("Term",seq(nTerms))
Topics <- paste("Topic", seq(K))
Documents <- paste("Document", seq(M))
## Generate latent topic and term distributions
# "True" Document/Topic distribution matrix
Theta <- rdirichlet(M, Alpha)
colnames(Theta) <- Topics
rownames(Theta) <- Documents
# "True" Topic/Term Distribution Matrix
Phi <- rdirichlet(K, Beta)
colnames(Phi) <- Terms
rownames(Phi) <- Topics
## Function to generate individual document
generateDoc <- function(docLength, topic_dist, terms_topics_dist){
# docLength is specific document length
# topic_dist is specific topic distribution for this document
# terms_topics_dist is terms distribution matrix over all topics
document <- c()
for (i in seq(docLength)){
# For each word in a document,
# choose a topic from that
# document's topic distribution
topic <- rmultinom(1, 1, topic_dist)
# Then choose a term from that topic's term distribution
term <- rmultinom(1, 1, terms_topics_dist[topic,])
# and append term to document vector
document <- c(document,
colnames(terms_topics_dist)[which.max(term)])
}
return(document)
}
## generate "observed" corpus as list of terms
corpus <- list()
for (i in seq(M)){
corpus[[i]] <- generateDoc(docLengths[i], Theta[i,], Phi)
}
## convert document term vectors to frequency vectors
freqsLists <- llply(corpus, table)
## write values to termFreqMatrix
termFreqMatrix <- matrix(nrow=M, ncol=nTerms, 0)
colnames(termFreqMatrix) <- Terms
rownames(termFreqMatrix) <- Documents
for (i in seq(M)){
termFreqMatrix[i,names(freqsLists[[i]])] <- freqsLists[[i]]
}
stopifnot(rowSums(termFreqMatrix) == docLengths)
return(list("docs"=corpus,
'termFreqMatrix'=termFreqMatrix,
"Theta"=Theta,
"Phi"=Phi))
}
simulateCorpus(M=10,nTerms=100,docLengths=60,K=2,alphA=0.2,betA=0.3,)
install.packages("LCA")
library(ggplot2)
library(tm)
library(MCMCpack)
simulateCorpus <- function(
require("LCA")
M, # number of documents
nTerms,
docLengths,
K,  	# Number of Topics
alphA, 	# parameter for symmetric
# Document/Topic dirichlet distribution
betA, 	# parameter for Topic/Term dirichlet distribution
Alpha=rep(alphA,K), # number-of-topics length vector
# set to symmetric alpha parameter
# across all topics
Beta=rep(betA,nTerms))  # number-of-terms length vector
# set to symmetric beta parameter
### Basic LDA Topic Model Simulation ###
### Generate Simulated Corpus ###
library(ggplot2)
library(tm)
library(MCMCpack)
simulateCorpus <- function(
M, # number of documents
nTerms,
docLengths,
K,  	# Number of Topics
alphA, 	# parameter for symmetric
# Document/Topic dirichlet distribution
betA, 	# parameter for Topic/Term dirichlet distribution
Alpha=rep(alphA,K), # number-of-topics length vector
# set to symmetric alpha parameter
# across all topics
Beta=rep(betA,nTerms))  # number-of-terms length vector
# set to symmetric beta parameter
# across all terms
{
require("LCA")
# Labels
Terms <- paste("Term",seq(nTerms))
Topics <- paste("Topic", seq(K))
Documents <- paste("Document", seq(M))
## Generate latent topic and term distributions
# "True" Document/Topic distribution matrix
Theta <- rdirichlet(M, Alpha)
colnames(Theta) <- Topics
rownames(Theta) <- Documents
# "True" Topic/Term Distribution Matrix
Phi <- rdirichlet(K, Beta)
colnames(Phi) <- Terms
rownames(Phi) <- Topics
## Function to generate individual document
generateDoc <- function(docLength, topic_dist, terms_topics_dist){
# docLength is specific document length
# topic_dist is specific topic distribution for this document
# terms_topics_dist is terms distribution matrix over all topics
document <- c()
for (i in seq(docLength)){
# For each word in a document,
# choose a topic from that
# document's topic distribution
topic <- rmultinom(1, 1, topic_dist)
# Then choose a term from that topic's term distribution
term <- rmultinom(1, 1, terms_topics_dist[topic,])
# and append term to document vector
document <- c(document,
colnames(terms_topics_dist)[which.max(term)])
}
return(document)
}
## generate "observed" corpus as list of terms
corpus <- list()
for (i in seq(M)){
corpus[[i]] <- generateDoc(docLengths[i], Theta[i,], Phi)
}
## convert document term vectors to frequency vectors
freqsLists <- llply(corpus, table)
## write values to termFreqMatrix
termFreqMatrix <- matrix(nrow=M, ncol=nTerms, 0)
colnames(termFreqMatrix) <- Terms
rownames(termFreqMatrix) <- Documents
for (i in seq(M)){
termFreqMatrix[i,names(freqsLists[[i]])] <- freqsLists[[i]]
}
stopifnot(rowSums(termFreqMatrix) == docLengths)
return(list("docs"=corpus,
'termFreqMatrix'=termFreqMatrix,
"Theta"=Theta,
"Phi"=Phi))
}
simulateCorpus(M=10,nTerms=100,docLengths=60,K=2,alphA=0.2,betA=0.3,)
### Basic LDA Topic Model Simulation ###
### Generate Simulated Corpus ###
library(ggplot2)
library(tm)
library(MCMCpack)
simulateCorpus <- function(
M, # number of documents
nTerms,
docLength,
K,  	# Number of Topics
alphA, 	# parameter for symmetric
# Document/Topic dirichlet distribution
betA, 	# parameter for Topic/Term dirichlet distribution
Alpha=rep(alphA,K), # number-of-topics length vector
# set to symmetric alpha parameter
# across all topics
Beta=rep(betA,nTerms))  # number-of-terms length vector
# set to symmetric beta parameter
# across all terms
{
require("LCA")
# Labels
Terms <- paste("Term",seq(nTerms))
Topics <- paste("Topic", seq(K))
Documents <- paste("Document", seq(M))
## Generate latent topic and term distributions
# "True" Document/Topic distribution matrix
Theta <- rdirichlet(M, Alpha)
colnames(Theta) <- Topics
rownames(Theta) <- Documents
# "True" Topic/Term Distribution Matrix
Phi <- rdirichlet(K, Beta)
colnames(Phi) <- Terms
rownames(Phi) <- Topics
## Function to generate individual document
generateDoc <- function(docLength, topic_dist, terms_topics_dist){
# docLength is specific document length
# topic_dist is specific topic distribution for this document
# terms_topics_dist is terms distribution matrix over all topics
document <- c()
for (i in seq(docLength)){
# For each word in a document,
# choose a topic from that
# document's topic distribution
topic <- rmultinom(1, 1, topic_dist)
# Then choose a term from that topic's term distribution
term <- rmultinom(1, 1, terms_topics_dist[topic,])
# and append term to document vector
document <- c(document,
colnames(terms_topics_dist)[which.max(term)])
}
return(document)
}
## generate "observed" corpus as list of terms
corpus <- list()
for (i in seq(M)){
corpus[[i]] <- generateDoc(docLengths[i], Theta[i,], Phi)
}
## convert document term vectors to frequency vectors
freqsLists <- llply(corpus, table)
## write values to termFreqMatrix
termFreqMatrix <- matrix(nrow=M, ncol=nTerms, 0)
colnames(termFreqMatrix) <- Terms
rownames(termFreqMatrix) <- Documents
for (i in seq(M)){
termFreqMatrix[i,names(freqsLists[[i]])] <- freqsLists[[i]]
}
stopifnot(rowSums(termFreqMatrix) == docLengths)
return(list("docs"=corpus,
'termFreqMatrix'=termFreqMatrix,
"Theta"=Theta,
"Phi"=Phi))
}
simulateCorpus(M=10,nTerms=100,docLength=60,K=2,alphA=0.2,betA=0.3,)
docLength
### Basic LDA Topic Model Simulation ###
### Generate Simulated Corpus ###
library(ggplot2)
library(tm)
library(MCMCpack)
simulateCorpus <- function(
M, # number of documents
nTerms,
docLengths,
K,  	# Number of Topics
alphA, 	# parameter for symmetric
# Document/Topic dirichlet distribution
betA, 	# parameter for Topic/Term dirichlet distribution
Alpha=rep(alphA,K), # number-of-topics length vector
# set to symmetric alpha parameter
# across all topics
Beta=rep(betA,nTerms))  # number-of-terms length vector
# set to symmetric beta parameter
# across all terms
{
require("LCA")
# Labels
Terms <- paste("Term",seq(nTerms))
Topics <- paste("Topic", seq(K))
Documents <- paste("Document", seq(M))
## Generate latent topic and term distributions
# "True" Document/Topic distribution matrix
Theta <- rdirichlet(M, Alpha)
colnames(Theta) <- Topics
rownames(Theta) <- Documents
# "True" Topic/Term Distribution Matrix
Phi <- rdirichlet(K, Beta)
colnames(Phi) <- Terms
rownames(Phi) <- Topics
## Function to generate individual document
generateDoc <- function(docLengths, topic_dist, terms_topics_dist){
# docLength is specific document length
# topic_dist is specific topic distribution for this document
# terms_topics_dist is terms distribution matrix over all topics
document <- c()
for (i in seq(docLengths)){
# For each word in a document,
# choose a topic from that
# document's topic distribution
topic <- rmultinom(1, 1, topic_dist)
# Then choose a term from that topic's term distribution
term <- rmultinom(1, 1, terms_topics_dist[topic,])
# and append term to document vector
document <- c(document,
colnames(terms_topics_dist)[which.max(term)])
}
return(document)
}
## generate "observed" corpus as list of terms
corpus <- list()
for (i in seq(M)){
corpus[[i]] <- generateDoc(docLengths[i], Theta[i,], Phi)
}
## convert document term vectors to frequency vectors
freqsLists <- llply(corpus, table)
## write values to termFreqMatrix
termFreqMatrix <- matrix(nrow=M, ncol=nTerms, 0)
colnames(termFreqMatrix) <- Terms
rownames(termFreqMatrix) <- Documents
for (i in seq(M)){
termFreqMatrix[i,names(freqsLists[[i]])] <- freqsLists[[i]]
}
stopifnot(rowSums(termFreqMatrix) == docLengths)
return(list("docs"=corpus,
'termFreqMatrix'=termFreqMatrix,
"Theta"=Theta,
"Phi"=Phi))
}
simulateCorpus(M=10,nTerms=100,docLengths=60,K=2,alphA=0.2,betA=0.3)
library(XLConnectJars)
library(XLConnect)
install.packages("XLConnect")
library(XLConnect)
install.packages("XLConnectJars")
library(XLConnectJars)
library(XLConnect)
wb <- loadWorkbook("frydata.xlsx")
install.packages("xlsx")
library(xlsx)
library(xlsx)
setwd("~/GitHub/Hierarchy_In_Networks")
source("./Scripts/calculate_analytical_hierarchy_measures.R")
source("./Scripts/score_leadership_rank.R")
source("./Scripts/multi_plot.R")
source('./Scripts/generate_hierarchy_dataset.R')
source('./Scripts/calculate_descriptive_statistics.R')
load("./Data/Network_Data.Rdata")
length(Network_Data[[-c(100,52:54,128,78)]])
length(Network_Data[-c(100,52:54,128,78)])
data_list <- generate_hierarchy_dataset(Network_Data[-c(100,52:54,128,78)])
global_measures <- data_list$global_measure_dataframe
multi_plot(data = global_measures,
pdf_name = "Global_Measures",
output_pdf = F,
c(2:5,7))
multi_plot(data = data_list$leadership_ranking_scores,
pdf_name = "Measure_Scores",
output_pdf = F)
descriptive_stats <- calculate_descriptive_statistics(Network_Data[-c(100,52:54,128,78)])
head(descriptive_stats)
library(ggbiplot)
library(devtools)
install.packages("ggbiplot")
