install.packages("LCA")
library(ggplot2)
library(tm)
library(MCMCpack)
simulateCorpus <- function(
require("LCA")
M, # number of documents
nTerms,
docLengths,
K,  	# Number of Topics
alphA, 	# parameter for symmetric
# Document/Topic dirichlet distribution
betA, 	# parameter for Topic/Term dirichlet distribution
Alpha=rep(alphA,K), # number-of-topics length vector
# set to symmetric alpha parameter
# across all topics
Beta=rep(betA,nTerms))  # number-of-terms length vector
# set to symmetric beta parameter
### Basic LDA Topic Model Simulation ###
### Generate Simulated Corpus ###
library(ggplot2)
library(tm)
library(MCMCpack)
simulateCorpus <- function(
M, # number of documents
nTerms,
docLengths,
K,  	# Number of Topics
alphA, 	# parameter for symmetric
# Document/Topic dirichlet distribution
betA, 	# parameter for Topic/Term dirichlet distribution
Alpha=rep(alphA,K), # number-of-topics length vector
# set to symmetric alpha parameter
# across all topics
Beta=rep(betA,nTerms))  # number-of-terms length vector
# set to symmetric beta parameter
# across all terms
{
require("LCA")
# Labels
Terms <- paste("Term",seq(nTerms))
Topics <- paste("Topic", seq(K))
Documents <- paste("Document", seq(M))
## Generate latent topic and term distributions
# "True" Document/Topic distribution matrix
Theta <- rdirichlet(M, Alpha)
colnames(Theta) <- Topics
rownames(Theta) <- Documents
# "True" Topic/Term Distribution Matrix
Phi <- rdirichlet(K, Beta)
colnames(Phi) <- Terms
rownames(Phi) <- Topics
## Function to generate individual document
generateDoc <- function(docLength, topic_dist, terms_topics_dist){
# docLength is specific document length
# topic_dist is specific topic distribution for this document
# terms_topics_dist is terms distribution matrix over all topics
document <- c()
for (i in seq(docLength)){
# For each word in a document,
# choose a topic from that
# document's topic distribution
topic <- rmultinom(1, 1, topic_dist)
# Then choose a term from that topic's term distribution
term <- rmultinom(1, 1, terms_topics_dist[topic,])
# and append term to document vector
document <- c(document,
colnames(terms_topics_dist)[which.max(term)])
}
return(document)
}
## generate "observed" corpus as list of terms
corpus <- list()
for (i in seq(M)){
corpus[[i]] <- generateDoc(docLengths[i], Theta[i,], Phi)
}
## convert document term vectors to frequency vectors
freqsLists <- llply(corpus, table)
## write values to termFreqMatrix
termFreqMatrix <- matrix(nrow=M, ncol=nTerms, 0)
colnames(termFreqMatrix) <- Terms
rownames(termFreqMatrix) <- Documents
for (i in seq(M)){
termFreqMatrix[i,names(freqsLists[[i]])] <- freqsLists[[i]]
}
stopifnot(rowSums(termFreqMatrix) == docLengths)
return(list("docs"=corpus,
'termFreqMatrix'=termFreqMatrix,
"Theta"=Theta,
"Phi"=Phi))
}
simulateCorpus(M=10,nTerms=100,docLengths=60,K=2,alphA=0.2,betA=0.3,)
### Basic LDA Topic Model Simulation ###
### Generate Simulated Corpus ###
library(ggplot2)
library(tm)
library(MCMCpack)
simulateCorpus <- function(
M, # number of documents
nTerms,
docLength,
K,  	# Number of Topics
alphA, 	# parameter for symmetric
# Document/Topic dirichlet distribution
betA, 	# parameter for Topic/Term dirichlet distribution
Alpha=rep(alphA,K), # number-of-topics length vector
# set to symmetric alpha parameter
# across all topics
Beta=rep(betA,nTerms))  # number-of-terms length vector
# set to symmetric beta parameter
# across all terms
{
require("LCA")
# Labels
Terms <- paste("Term",seq(nTerms))
Topics <- paste("Topic", seq(K))
Documents <- paste("Document", seq(M))
## Generate latent topic and term distributions
# "True" Document/Topic distribution matrix
Theta <- rdirichlet(M, Alpha)
colnames(Theta) <- Topics
rownames(Theta) <- Documents
# "True" Topic/Term Distribution Matrix
Phi <- rdirichlet(K, Beta)
colnames(Phi) <- Terms
rownames(Phi) <- Topics
## Function to generate individual document
generateDoc <- function(docLength, topic_dist, terms_topics_dist){
# docLength is specific document length
# topic_dist is specific topic distribution for this document
# terms_topics_dist is terms distribution matrix over all topics
document <- c()
for (i in seq(docLength)){
# For each word in a document,
# choose a topic from that
# document's topic distribution
topic <- rmultinom(1, 1, topic_dist)
# Then choose a term from that topic's term distribution
term <- rmultinom(1, 1, terms_topics_dist[topic,])
# and append term to document vector
document <- c(document,
colnames(terms_topics_dist)[which.max(term)])
}
return(document)
}
## generate "observed" corpus as list of terms
corpus <- list()
for (i in seq(M)){
corpus[[i]] <- generateDoc(docLengths[i], Theta[i,], Phi)
}
## convert document term vectors to frequency vectors
freqsLists <- llply(corpus, table)
## write values to termFreqMatrix
termFreqMatrix <- matrix(nrow=M, ncol=nTerms, 0)
colnames(termFreqMatrix) <- Terms
rownames(termFreqMatrix) <- Documents
for (i in seq(M)){
termFreqMatrix[i,names(freqsLists[[i]])] <- freqsLists[[i]]
}
stopifnot(rowSums(termFreqMatrix) == docLengths)
return(list("docs"=corpus,
'termFreqMatrix'=termFreqMatrix,
"Theta"=Theta,
"Phi"=Phi))
}
simulateCorpus(M=10,nTerms=100,docLength=60,K=2,alphA=0.2,betA=0.3,)
docLength
### Basic LDA Topic Model Simulation ###
### Generate Simulated Corpus ###
library(ggplot2)
library(tm)
library(MCMCpack)
simulateCorpus <- function(
M, # number of documents
nTerms,
docLengths,
K,  	# Number of Topics
alphA, 	# parameter for symmetric
# Document/Topic dirichlet distribution
betA, 	# parameter for Topic/Term dirichlet distribution
Alpha=rep(alphA,K), # number-of-topics length vector
# set to symmetric alpha parameter
# across all topics
Beta=rep(betA,nTerms))  # number-of-terms length vector
# set to symmetric beta parameter
# across all terms
{
require("LCA")
# Labels
Terms <- paste("Term",seq(nTerms))
Topics <- paste("Topic", seq(K))
Documents <- paste("Document", seq(M))
## Generate latent topic and term distributions
# "True" Document/Topic distribution matrix
Theta <- rdirichlet(M, Alpha)
colnames(Theta) <- Topics
rownames(Theta) <- Documents
# "True" Topic/Term Distribution Matrix
Phi <- rdirichlet(K, Beta)
colnames(Phi) <- Terms
rownames(Phi) <- Topics
## Function to generate individual document
generateDoc <- function(docLengths, topic_dist, terms_topics_dist){
# docLength is specific document length
# topic_dist is specific topic distribution for this document
# terms_topics_dist is terms distribution matrix over all topics
document <- c()
for (i in seq(docLengths)){
# For each word in a document,
# choose a topic from that
# document's topic distribution
topic <- rmultinom(1, 1, topic_dist)
# Then choose a term from that topic's term distribution
term <- rmultinom(1, 1, terms_topics_dist[topic,])
# and append term to document vector
document <- c(document,
colnames(terms_topics_dist)[which.max(term)])
}
return(document)
}
## generate "observed" corpus as list of terms
corpus <- list()
for (i in seq(M)){
corpus[[i]] <- generateDoc(docLengths[i], Theta[i,], Phi)
}
## convert document term vectors to frequency vectors
freqsLists <- llply(corpus, table)
## write values to termFreqMatrix
termFreqMatrix <- matrix(nrow=M, ncol=nTerms, 0)
colnames(termFreqMatrix) <- Terms
rownames(termFreqMatrix) <- Documents
for (i in seq(M)){
termFreqMatrix[i,names(freqsLists[[i]])] <- freqsLists[[i]]
}
stopifnot(rowSums(termFreqMatrix) == docLengths)
return(list("docs"=corpus,
'termFreqMatrix'=termFreqMatrix,
"Theta"=Theta,
"Phi"=Phi))
}
simulateCorpus(M=10,nTerms=100,docLengths=60,K=2,alphA=0.2,betA=0.3)
library(XLConnectJars)
library(XLConnect)
install.packages("XLConnect")
library(XLConnect)
install.packages("XLConnectJars")
library(XLConnectJars)
library(XLConnect)
wb <- loadWorkbook("frydata.xlsx")
install.packages("xlsx")
library(xlsx)
library(xlsx)
calculate_analytical_hierarhy_measures <- function(sociomatrix,
mode = "directed"){
#statistics using igraph
require(igraph)
adjacency <- sociomatrix
sociomatrix <- graph.adjacency(sociomatrix, mode = mode)
isDirected <- FALSE
if(mode == "directed"){
isDirected <- TRUE
}
#calculate global scores
global <- list()
global$degree_centralization <- centralization.degree (sociomatrix, mode = "all")$centralization
global$closeness_centralization <- centralization.closeness (sociomatrix, mode = "all")$centralization
global$betweenness_centralization <-  centralization.betweenness (sociomatrix, directed = isDirected)$centralization
global$eigenvector_centralization <- centralization.evcent (sociomatrix, directed = isDirected)$centralization
if(isDirected){
global$landau <- landau(adjacency)$global
global$kendall <- kendall(adjacency)$global
global$GRC <- GRC(adjacency)$global
global$D_root <- D_root(adjacency)$global
}
#calculate local scores
local <- list()
local$degree_centrality$score <- centralization.degree (sociomatrix, mode = "all")$res
local$degree_centrality$rank <- order(local$degree_centrality$score, decreasing = T)
local$closeness_centrality$score <- centralization.closeness (sociomatrix, mode = "all")$res
local$closeness_centrality$rank <- order(local$closeness_centrality$score, decreasing = T)
local$betweenness_centrality$score <-  centralization.betweenness (sociomatrix, directed = isDirected)$res
local$betweenness_centrality$rank <- order(local$betweenness_centrality$score, decreasing = T)
local$eigenvector_centrality$score <- centralization.evcent (sociomatrix, directed = isDirected)$vector
local$eigenvector_centrality$rank <- order(local$eigenvector_centrality$score, decreasing = T)
if(isDirected){
local$m_degree$score <- m_degree(adjacency)$local
local$m_degree$rank <- order(local$m_degree$score, decreasing = T)
local$m_close$score <- m_close(adjacency)$local
local$m_close$rank <- order(local$m_close$score, decreasing = T)
local$GRC$score <- GRC(adjacency)$local
local$GRC$rank <- order(local$GRC$score, decreasing = T)
local$D_root$score <- D_root(adjacency)$local
if(!is.na(local$D_root$score[1])){
local$D_root$rank <- order(local$D_root$score, decreasing = T)
}else{
local$D_root$rank <- NA
}
}
# return a list object with a $global and $local sublist, each of which contains the output from all of the different measures which are appropriately named. In the $local sublist, we provide the $rank and $score for each node in the network.
return_list <- list(global = global, local = local)
return(return_list)
}
#######################################################################################
#Landau's h-outputs global stat only works for directed graphs
landau <- function(matrix,directed=TRUE){
if(directed==FALSE){
print("error: this measure may only be used with directed networks")
break;
}
N=nrow(matrix)
S=apply(matrix,1,sum)
sum(S-((N-1)/2))
h=12/(N^3-N) * sum(S-((N-1)/2))
results=list(global=h,local=rep(NA,N))
return(results)
}
#######################################################################################
#Kendall's K-outputs global stat only works for directed graphs
kendall <- function(matrix,directed=TRUE){
if(directed==FALSE){
print("error: this measure may only be used with directed networks")
break;
}
N=nrow(matrix)
S=apply(matrix,1,sum)
d=(N*(N-1)*(2*N-1))/12-(0.5*sum(S^2))
if((N%%2)==0){
#even
d_max=(1/24)*(N^3-4*N)
} else{
#odd
d_max=(1/24)*(N^3-N)
}
K=1-(d/d_max)
results=list(global=K,local=rep(NA,N))
return(results)
}
#######################################################################################
#m reach degree-requires the keyplayer package in R; outputs local stats
m_degree <- function(matrix,directed=TRUE){
require("keyplayer")
l=mreach.degree(matrix,cmode="outdegree")
results=list(global=NA,local=l)
return(results)
}
#######################################################################################
#m reach closeness-requires the keyplayer package in R; outputs local stats
m_close <- function(matrix,directed=TRUE){
require("keyplayer")
l=mreach.closeness(matrix,cmode="outdegree")
results=list(global=NA,local=l)
return(results)
}
#######################################################################################
#GRC-requires the keyplayer package in R; outputs global and local stats
GRC <- function(matrix,directed=TRUE){
require("keyplayer")
N=dim(matrix)[1]
if(directed==TRUE){
C=mreach.degree(matrix,cmode="outdegree")
}
if(directed==FALSE){
C=mreach.degree(matrix,cmode="all")[,3]
}
C_max=max(C)
GRC=(sum(C_max-C))/(N-1)
results=list(global=GRC,local=C)
return(results)
}
#######################################################################################
#Rooted Depth-requires the igraph package in R; outputs global and local stats
D_root <- function(matrix,directed=TRUE){
require(igraph)
if(directed==FALSE){
print("error: this measure may only be used with directed networks")
break;
}
roots=which(apply(matrix,1,sum)==0)
if(length(roots)==0){
print("There are no roots in your network. Cannot calculate measure.")
results=list(global=NA,local=NA)
return(results)
}else{
N=dim(matrix)[1]
graph=graph_from_adjacency_matrix(matrix,mode="directed")
paths=as.matrix(shortest.paths(graph,v=roots))
l=apply(paths,2,mean)
global_val =mean(as.vector(paths))
if(is.finite(global_val)){
results=list(global=global_val,local=l)
}else{
results=list(global=NA,local=l)
}
return(results)
}
}
multi_plot <- function(data,
pdf_name = NULL,
output_pdf = F,
plot_columns = 1:ncol(data),
legend_location = "bottomleft",
xlabel = "Network",
ylabel = "Score",
category = "Measure"){
UMASS_BLUE <- rgb(51,51,153,195,maxColorValue = 255)
UMASS_RED <- rgb(153,0,51,195,maxColorValue = 255)
UMASS_GREEN <- rgb(0,102,102,195,maxColorValue = 255)
UMASS_YELLOW <- rgb(255,255,102,255,maxColorValue = 255)
UMASS_ORANGE <- rgb(255,204,51,195,maxColorValue = 255)
colors <- c(UMASS_BLUE,UMASS_RED, UMASS_GREEN, UMASS_ORANGE, UMASS_YELLOW)
plot_colors <- NULL
index <- 1
for(i in 1:length(plot_columns)){
plot_colors <- c(plot_colors, colors[index])
index <- index + 1
if(index == 6){
index <- 1
}
}
if(output_pdf){
pdf(file = paste("./Output/",pdf_name,".pdf",sep = ""),height=5, width=8, family="Times", pointsize=13.5)
par(las=2,mar=c(6,5,.5,.5),cex.lab=.5,xpd=TRUE)
plot(data[,plot_columns[1]],ylim=c(min(data[,plot_columns],0),max(data[,plot_columns],1)),lwd=-1,col = "white",type="l",xaxt= "n",xlab=xlabel,ylab=ylabel, main = "",cex.axis = 0.5)
for(i in 1:nrow(data)){
segments(x0=i,y0=min(data[,plot_columns],0),x1=i,y1=max(data[,plot_columns],1),lwd=0.5,col="grey80",lty=3)
}
# abline(v=1:nrow(data),ylim=c(min(data[,plot_columns],0),max(data[,plot_columns],1)),lwd=0.5,col="grey80",lty=3)
col_names <- NULL
col_colors <- NULL
col_shapes <- NULL
for(i in 1:length(plot_columns)){
points(data[,plot_columns[i]] ~ jitter(1:nrow(data),0.3),
pch=14+i,
col=plot_colors[i],
bg=plot_colors[i])
col_names <- c(col_names,colnames(data)[plot_columns[i]])
col_colors <- c(col_colors,plot_colors[i])
col_shapes <- c(col_shapes,14+i)
}
axis(1, col='black', at=1:nrow(data), labels=rownames(data),lwd= 1, cex.axis = 0.5)
legend(legend_location,
legend=col_names,
pch=col_shapes ,
col = col_colors,
title=category,
cex = 0.5)
dev.off()
}else{
par(las=2,mar=c(6,5,.5,.5),cex.lab=.5,xpd=TRUE)
plot(data[,plot_columns[1]],ylim=c(min(data[,plot_columns],0),max(data[,plot_columns],1)),lwd=0,type="l",xaxt= "n",xlab=xlabel,ylab=ylabel, main = "",cex.axis = 0.5)
for(i in 1:nrow(data)){
segments(x0=i,y0=min(data[,plot_columns],0),x1=i,y1=max(data[,plot_columns],1),lwd=0.5,col="grey80",lty=3)
}
# abline(v=1:nrow(data),ylim=c(min(data[,plot_columns],0),max(data[,plot_columns],1)),lwd=0.5,col="grey80",lty=3)
col_names <- NULL
col_colors <- NULL
col_shapes <- NULL
for(i in 1:length(plot_columns)){
points(data[,plot_columns[i]],pch=14+i,col=plot_colors[i],bg=plot_colors[i])
col_names <- c(col_names,colnames(data)[plot_columns[i]])
col_colors <- c(col_colors,plot_colors[i])
col_shapes <- c(col_shapes,14+i)
}
axis(1, col='black', at=1:nrow(data), labels=rownames(data),lwd= 1, cex.axis = 0.5)
legend(legend_location,
legend=col_names,
pch=col_shapes ,
col = col_colors,
title=category,
cex = 0.5)
}
}
source("./Scripts/calculate_analytical_hierarchy_measures.R")
source("./Scripts/score_leadership_rank.R")
source("./Scripts/multi_plot.R")
source('./Scripts/generate_hierarchy_dataset.R')
source('./Scripts/calculate_descriptive_statistics.R')
# for better plotting you can try this
load("~/GitHub/Hierarchy_In_Networks/Data/global_hierarchy_measures.Rdata")
head(global_measures)
rownames(global_measures)
help(pairs)
attach(iris)
pairs(iris[1:4], main = "Anderson's Iris Data -- 3 species",
pch = 21, bg = c("red", "green3", "blue")[unclass(iris$Species)])
load(global_hierarchy_measures.Rdata)
setwd("~/GitHub/Hierarchy_In_Networks")
load(global_hierarchy_measures.Rdata)
load("./Data/global_hierarchy_measures.Rdata")
load("~/GitHub/Hierarchy_In_Networks/Data/Network_Data.Rdata")
length(Network_Data)
length(Network_Data[[1]])
Network_Data[[1]]
hold=rep(NA,length(Network_Data))
for(i in 1:length(Network_Data)){
hold[i]=c(Network_Data[[i]]$type)
}
hold
as.factor(hold)
length(Network_Data)
head(global_measures)
rownames(global_meausres)
rownames(global_measures)
which(rownames(global_measures)=="CITIES")
which(rownames(global_measures)=="drugnet")
data_list <- generate_hierarchy_dataset(Network_Data)
source("./Scripts/calculate_analytical_hierarchy_measures.R")
source("./Scripts/score_leadership_rank.R")
source("./Scripts/multi_plot.R")
source('./Scripts/generate_hierarchy_dataset.R')
source('./Scripts/calculate_descriptive_statistics.R')
data_list <- generate_hierarchy_dataset(Network_Data)
